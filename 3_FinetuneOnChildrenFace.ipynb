{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)获得ADOS的dataset class\n",
    "把获得的7frames转成tensor，并储存为list格式的sampleLib。\n",
    "\n",
    "储存的tensor已经对图片进行了transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_tensor(input_video):\n",
    "    transform=transforms.Compose([  transforms.ToPILImage(),\n",
    "    transforms.Resize(256),  \n",
    "                                transforms.CenterCrop(224),  # crop到224\n",
    "                                transforms.ToTensor()])\n",
    "    tensor_list = []\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    for i in range(int(cap.get(7))):\n",
    "        ret, frame = cap.read()\n",
    "        frame_trans = transform(frame)\n",
    "        frame_trans = torch.unsqueeze(frame_trans, 0)\n",
    "        tensor_list.append(frame_trans)\n",
    "    video_tensor = torch.cat(tensor_list,dim=0) \n",
    "    return video_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /AsdData/pany/ados_mix_folder/11/20190516-黄翰_466.avi NonSmile\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    root_path = '/AsdData/pany/ados_mix_folder/' \n",
    "    label_dict = {'Smile':0,'NonSmile':1}\n",
    "    for i in range(12):\n",
    "        file = os.path.join(root_path,'{}/label.txt'.format(i))      \n",
    "        video_list = []\n",
    "        \n",
    "        for line in open(file):  \n",
    "            [video_path,label_class] = line.split()\n",
    "            if label_class in ['Smile','NonSmile'] :\n",
    "                tensor_list  = video_tensor(video_path)\n",
    "                label = label_dict[label_class]\n",
    "                if int(tensor_list.shape[0]) !=7:\n",
    "                    print(video_path)\n",
    "                video_list.append([tensor_list,label])\n",
    "        torch.save(video_list, '/AsdData/pany/RSS_checkpoints/pt14/ados_video_tensor_list_{}.pkl'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    root_path = '/AsdData/pany/ados_mix_folder/' \n",
    "    label_dict = {'Smile':0,'NonSmile':1}\n",
    "    for i in range(12):\n",
    "        file = os.path.join(root_path,'{}/label.txt'.format(i))      \n",
    "        video_list = []\n",
    "        \n",
    "        for line in open(file):  \n",
    "            [video_path,label_class] = line.split()\n",
    "            if label_class in ['Smile','NonSmile'] :\n",
    "                tensor_list  = video_tensor(video_path)\n",
    "                label = label_dict[label_class]\n",
    "                if int(tensor_list.shape[0]) !=7:\n",
    "                    print(video_path)\n",
    "                video_list.append([tensor_list,label])\n",
    "        torch.save(video_list, '/AsdData/pany/RSS_checkpoints/pt14/ados_video_tensor_list_{}.pkl'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = torch.load('/AsdData/pany/RSS_checkpoints/pt14/ados_video_tensor_list_1.pkl')\n",
    "list_2 = torch.load('/AsdData/pany/RSS_checkpoints/pt14/ados_video_tensor_list_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1.extend(list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    video_list = []\n",
    "    for i in range(11):\n",
    "        file_path = '/AsdData/pany/RSS_checkpoints/pt14/ados_video_tensor_list_{}.pkl'.format(i)    \n",
    "        list_i = torch.load(file_path)\n",
    "        video_list.extend(list_i)\n",
    "    print(len(video_list))\n",
    "    torch.save(video_list, '/AsdData/pany/RSS_checkpoints/pt14/ados_video_tensor_list_no_11.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)导入pretrained模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "# import dlib\n",
    "import time\n",
    "import sys\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable  \n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset \n",
    "import numpy as np\n",
    "from torch.utils import data as Data\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from ops.basic_ops import ConsensusModule\n",
    "from tfennet14 import *\n",
    "# get_ipython().run_line_magic('matplotlib', 'notebook')\n",
    "\n",
    "import visdom\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Without the incoming socket you cannot receive events from the server or register event handlers to your Visdom client.\n"
     ]
    }
   ],
   "source": [
    "vis =  visdom.Visdom(env=u'pytorch14',use_incoming_socket=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydataset(data.Dataset):\n",
    "    '''\n",
    "    the format of sampleLib:[tensor,label],[tensor,label],[tensor,label].......\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,sampleLib):\n",
    "        self.sampleLib=sampleLib\n",
    "#         self.transform = transforms.Compose([\n",
    "#                                                transforms.ToTensor()\n",
    "#                                            ])\n",
    "    def __getitem__(self,index):\n",
    "        [sample,label] = self.sampleLib[index]\n",
    "        return sample, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sampleLib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_model(pretrained_file, model):\n",
    "    '''\n",
    "    只导入pretrained_file部分模型参数\n",
    "    tensor([-0.7119,  0.0688, -1.7247, -1.7182, -1.2161, -0.7323, -2.1065, -0.5433,-1.5893, -0.5562]\n",
    "    update:\n",
    "        D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
    "        If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
    "        If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
    "        In either case, this is followed by: for k in F:  D[k] = F[k]\n",
    "    :param pretrained_file:\n",
    "    :param model:\n",
    "    :return:\n",
    "    '''\n",
    "    pretrained_dict = torch.load(pretrained_file)  # get pretrained dict\n",
    "    model_dict = model.state_dict()  # get model dict\n",
    "    # 在合并前(update),需要去除pretrained_dict一些不需要的参数\n",
    "    pretrained_dict = transfer_state_dict(pretrained_dict, model_dict)\n",
    "    model_dict.update(pretrained_dict)  # 更新(合并)模型的参数\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "def transfer_state_dict(pretrained_dict, model_dict):\n",
    "    '''\n",
    "    根据model_dict,去除pretrained_dict一些不需要的参数,以便迁移到新的网络\n",
    "    url: https://blog.csdn.net/qq_34914551/article/details/87871134\n",
    "    :param pretrained_dict:\n",
    "    :param model_dict:\n",
    "    :return:\n",
    "    '''\n",
    "    # state_dict2 = {k: v for k, v in save_model.items() if k in model_dict.keys()}\n",
    "    state_dict = {}\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict.keys():\n",
    "            # state_dict.setdefault(k, v)\n",
    "            state_dict[k] = v\n",
    "        else:\n",
    "            print(\"Missing key(s) in state_dict :{}\".format(k))\n",
    "    return state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, lr):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every SSTEP epochs\"\"\"\n",
    "    lr = lr * (0.01 ** (epoch // sstep))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def validation(test_loader,net):\n",
    "    '''\n",
    "    注意，此处有几个修改，test_y_list的写法修改过了\n",
    "    '''\n",
    "    pred_list = []\n",
    "    test_y_list = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        test_y_list.append(test_dataset[i][1])\n",
    "    #     test_loader.size\n",
    "    for step, (test_x,test_y) in enumerate(test_loader):\n",
    "#         test_x = test_x.to(DEVICE)\n",
    "        test_x = test_x.cuda()\n",
    "        test_y =test_y.numpy()\n",
    "\n",
    "        test_output = net(test_x)                   \n",
    "        pred_y = torch.max(test_output, 1)[1].data.cpu().numpy()\n",
    "\n",
    "        pred_list +=  list(pred_y  )\n",
    "\n",
    "    pred_ay = np.array(pred_list)\n",
    "    test_y_ay = np.array(test_y_list)\n",
    "\n",
    "    accu = float((pred_ay ==test_y_ay).astype(int).sum())/test_y_ay.size\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_model(net_2,net_6_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_2 = TFEN_2()\n",
    "net_6_pretrain = torch.load('torch14_oulu_netall.pkl')\n",
    "net_6_dict= net_6_pretrain.state_dict()\n",
    "\n",
    "net_2_dict = net_2.state_dict()\n",
    "net_2_dict_trans = transfer_state_dict(net_6_para_state, net_2_dict)\n",
    "\n",
    "net_2_dict.update(net_2_dict_trans)\n",
    "net_2.load_state_dict(net_2_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading data\n",
      "finish loading data\n",
      "Step:0, Accuracy Rate:0.4607142857142857\n",
      "Step:50, Accuracy Rate:0.65\n",
      "Step:100, Accuracy Rate:0.6226190476190476\n",
      "Step:150, Accuracy Rate:0.7571428571428571\n",
      "Step:200, Accuracy Rate:0.775\n",
      "Step:250, Accuracy Rate:0.7345238095238096\n",
      "Step:300, Accuracy Rate:0.8178571428571428\n",
      "Step:350, Accuracy Rate:0.8214285714285714\n",
      "Step:400, Accuracy Rate:0.8095238095238095\n",
      "Step:450, Accuracy Rate:0.7511904761904762\n",
      "Step:500, Accuracy Rate:0.8583333333333333\n",
      "Step:550, Accuracy Rate:0.8571428571428571\n",
      "Step:600, Accuracy Rate:0.8726190476190476\n",
      "Step:0, Accuracy Rate:0.6666666666666666\n",
      "Step:50, Accuracy Rate:0.85\n",
      "Step:100, Accuracy Rate:0.8321428571428572\n",
      "Step:150, Accuracy Rate:0.8642857142857143\n",
      "Step:200, Accuracy Rate:0.844047619047619\n",
      "Step:250, Accuracy Rate:0.8666666666666667\n",
      "Step:300, Accuracy Rate:0.8464285714285714\n",
      "Step:350, Accuracy Rate:0.8714285714285714\n",
      "Step:400, Accuracy Rate:0.8464285714285714\n",
      "Step:450, Accuracy Rate:0.8726190476190476\n",
      "Step:500, Accuracy Rate:0.8773809523809524\n",
      "Step:550, Accuracy Rate:0.9130952380952381\n",
      "Step:600, Accuracy Rate:0.9\n",
      "Step:0, Accuracy Rate:0.8011904761904762\n",
      "Step:50, Accuracy Rate:0.8940476190476191\n",
      "Step:100, Accuracy Rate:0.8821428571428571\n",
      "Step:150, Accuracy Rate:0.8630952380952381\n",
      "Step:200, Accuracy Rate:0.8738095238095238\n",
      "Step:250, Accuracy Rate:0.8869047619047619\n",
      "Step:300, Accuracy Rate:0.8845238095238095\n",
      "Step:350, Accuracy Rate:0.8738095238095238\n",
      "Step:400, Accuracy Rate:0.8380952380952381\n",
      "Step:450, Accuracy Rate:0.9047619047619048\n",
      "Step:500, Accuracy Rate:0.8952380952380953\n",
      "Step:550, Accuracy Rate:0.9071428571428571\n",
      "Step:600, Accuracy Rate:0.8857142857142857\n",
      "Step:0, Accuracy Rate:0.8773809523809524\n",
      "Step:50, Accuracy Rate:0.9202380952380952\n",
      "Step:100, Accuracy Rate:0.8511904761904762\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation\n",
    "time_start=time.time()\n",
    "\n",
    "\n",
    "loss_10 = []\n",
    "accu_10 = []\n",
    "net_10 = []\n",
    "group_list = [11]\n",
    "for i in group_list:\n",
    "    BATCH_SIZE =16\n",
    "    print('start loading data')\n",
    "    train_dataset = torch.load('/AsdData/pany/RSS_checkpoints/pt14/ados_video_tensor_list_no_11.pkl')\n",
    "    \n",
    "    train_loader = Data.DataLoader(\n",
    "        dataset=train_dataset,      \n",
    "        batch_size=BATCH_SIZE,      \n",
    "        shuffle = False,               \n",
    "    )\n",
    "    test_dataset = torch.load('/AsdData/pany/RSS_checkpoints/pt14/ados_video_tensor_list_{}.pkl'.format(i))\n",
    "    test_loader = Data.DataLoader(\n",
    "        dataset=test_dataset,     \n",
    "        batch_size=BATCH_SIZE,     \n",
    "        shuffle = False,               \n",
    "    )\n",
    "    print('finish loading data')\n",
    "    net = net_2\n",
    "#     net = torch.nn.DataParallel(net.cuda(), device_ids=[3])\n",
    "    net = net.cuda()\n",
    "    \n",
    "    EPOCH = 400\n",
    "    LR = 0.001\n",
    "    MOMENTUM = 0.9\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    sstep = 70\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    loss_his = []\n",
    "    accu_list = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(EPOCH):\n",
    "        \n",
    "#         print('====FOLDER:{}====EPOCH:{}/{}===='.format(i,epoch,EPOCH))\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch, LR)\n",
    "        \n",
    "        for step, (batch_x,batch_y) in enumerate(train_loader):\n",
    "\n",
    "            batch_x = batch_x.cuda()\n",
    "            batch_y = batch_y.cuda()\n",
    "\n",
    "            out = net(batch_x)\n",
    "            loss = loss_func(out,batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_his.append(loss.data.cpu().numpy())\n",
    "                        \n",
    "            if step % 50 == 0:\n",
    "                accuracy = validation(test_loader,net)\n",
    "                accu_list.append(accuracy)\n",
    "                print('Step:{}, Accuracy Rate:{}'.format(step,accuracy))\n",
    "                \n",
    "                \n",
    "                \n",
    "                if accuracy == max(accu_list):\n",
    "                    net_best = net\n",
    "                \n",
    "                \n",
    "                \n",
    "                x= torch.Tensor([len(accu_list)])\n",
    "                y=torch.Tensor([accuracy])\n",
    "                vis.line(X=x, Y=y, \n",
    "                         win='ados_finetune_{}'.format(i), \n",
    "                    opts=dict(\n",
    "                        title='ados_finetune_{}'.format(i),\n",
    "                        width=800,\n",
    "                        height=800,\n",
    "                             ),\n",
    "                         update='append' if len(accu_list)>0 else None)\n",
    "\n",
    "\n",
    "\n",
    "    loss_10.append(loss_his)\n",
    "    accu_10.append(accu_list)\n",
    "    net_10.append(net_best)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "time_end=time.time()\n",
    "m, s = divmod(time_end-time_start, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print (\"%02d:%02d:%02d\" % (h, m, s))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "m, s = divmod(time_end-time_start, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print (\"%02d:%02d:%02d\" % (h, m, s))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "torch.save(net_10,'torch14_adjustlr0.01_sstep70_finetune_ados.pkl')\n",
    "torch.save(accu_10,'torch14_adjustlr0.01_sstep70_finetune_ados.pkl')\n",
    "torch.save(loss_10,'torch14_adjustlr0.01_sstep70_finetune_ados.pkl')\n",
    "print(time.strftime('torch14_adjustlr0.01_sstep70_finetune_ados.pkl ===> %Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
