{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Prepare on Oulu dataset\n",
    "__[Dataset Description](http://www.ee.oulu.fi/~gyzhao/Download/Databases/NIR_VL_FED/Description.pdf)__\n",
    "\n",
    "(1) Extract face from the original video and randomly select 7 frames as a new video.\n",
    "\n",
    "* input_folder_path = '/.../Oulu-CASIA/A_OriginalVideo/VL/S'\n",
    "* output_folder_path = '/.../oulu_face_extracted'\n",
    "* ExtractFaceMMOD_Oulu7Frames.py\n",
    "\n",
    "(2)Transform 7 frames to a tensor, and save as a sampleLib list. \n",
    "\n",
    "* input_folder_path = '/.../oulu_face_extracted'\n",
    "* output_path = '/.../oulu_trans_video_list.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np\n",
    "import threading\n",
    "from random import sample\n",
    "\n",
    "\n",
    "class FaceExtractor():\n",
    "    def detect_face(self, frame):\n",
    "       \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            net: face detector\n",
    "            frame: image for face detection\n",
    "        Returns:\n",
    "            flag: whether there is a face\n",
    "            face_patch: cropped face region\n",
    "        \"\"\"\n",
    "        inHeight = 300\n",
    "\n",
    "        dnnFaceDetector = dlib.cnn_face_detection_model_v1(\"./models/mmod_human_face_detector.dat\")\n",
    "        \n",
    "        # Load face detector and process the frame\n",
    "        frameDlibMMOD = frame.copy()\n",
    "        frameHeight = frameDlibMMOD.shape[0]\n",
    "        frameWidth = frameDlibMMOD.shape[1]\n",
    "        inWidth = int((frameWidth / frameHeight)*inHeight)\n",
    "        scaleHeight = frameHeight / inHeight\n",
    "        scaleWidth = frameWidth / inWidth\n",
    "        \n",
    "        frameDlibMMODSmall = cv2.resize(frameDlibMMOD, (inWidth, inHeight))\n",
    "        frameDlibMMODSmall = cv2.cvtColor(frameDlibMMODSmall, cv2.COLOR_BGR2RGB)\n",
    "        faceRects = dnnFaceDetector(frameDlibMMODSmall, 0)\n",
    "        # Traverse the detection results\n",
    "    \n",
    "        \"\"\"\n",
    "        # extract face region from background\n",
    "        face_patch = frameDlibMMO[y1:y2, x1:x2, :]\n",
    "    \n",
    "        \"\"\"\n",
    "        face_patch = None\n",
    "        for faceRect in faceRects:\n",
    "            if  faceRect is not None:\n",
    "                cvRect = [int(faceRect.rect.left()*scaleWidth), int(faceRect.rect.top()*scaleHeight),                  int(faceRect.rect.right()*scaleWidth), int(faceRect.rect.bottom()*scaleHeight) ]\n",
    "                \n",
    "                x1 = cvRect[1]\n",
    "                y1 = cvRect[0]\n",
    "                x2 = cvRect[3]\n",
    "                y2 = cvRect[2]\n",
    "                \n",
    "                # extract face region from background\n",
    "                face_patch = frameDlibMMOD[x1:x2, y1:y2, :]\n",
    "                return face_patch\n",
    "                \n",
    "                \n",
    "    def extract_face(self, input_video, output_folder,  resolution):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_video: path of given video\n",
    "            output_path: target dir to save extracted video\n",
    "            conf_thresh: threshold for face detection\n",
    "            resolution: frame size of extracted face region\n",
    "        \"\"\"\n",
    "        # get save path \n",
    "        video_name = os.path.split(input_video.split('.')[0])[-1]\n",
    "        save_path = os.path.join(output_folder, video_name+'.avi')\n",
    "        # build video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "        VideoWriter = cv2.VideoWriter(filename=save_path,fourcc=fourcc,fps=30,frameSize=resolution, isColor=True)\n",
    "        # Load video \n",
    "        cap = cv2.VideoCapture(input_video)\n",
    "        video_len = (cap.get(7))\n",
    "        if int(video_len) == 7:\n",
    "            ls_7frame = [0,1,2,3,4,5,6]\n",
    "        else:\n",
    "            population = range(int(cap.get(7))-1)\n",
    "            ls_7frame = (sample(population,7))  # order num for 7 frames\n",
    "            ls_7frame = sorted(ls_7frame)\n",
    "        print(ls_7frame)\n",
    "        for i in range(int(cap.get(7))):\n",
    "            ret, frame = cap.read()\n",
    "            face = self.detect_face(frame)\n",
    "#             print(face)\n",
    "            \n",
    "            # resize and write frame into video, if there is a face\n",
    "            if face is not None: \n",
    "                \n",
    "                try:\n",
    "                    face = cv2.resize(face, resolution)\n",
    "                    \n",
    "                    if i in ls_7frame:\n",
    "                        print(i)\n",
    "                        VideoWriter.write(face)\n",
    "                except:\n",
    "                    pass\n",
    "        cap.release()\n",
    "        VideoWriter.release()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def run(self, input_video, output_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_list: paths of input videos\n",
    "            output_dir: target folder to save extracted videos\n",
    "        \"\"\"\n",
    "        # Extract face region from each video \n",
    "#         for i in video_list:\n",
    "        self.extract_face(input_video, output_folder=output_dir,resolution=(224,224))\n",
    "            \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    input_folder_path = '/.../Oulu-CASIA/A_OriginalVideo/VL'\n",
    "    output_folder_path = '/.../oulu_face_extracted'\n",
    "\n",
    "    face_extractor = FaceExtractor()\n",
    "\n",
    "\n",
    "    for root,dirs,files in os.walk(input_folder_path):\n",
    "        for file in files:\n",
    "            if file[8] == 'S':\n",
    "                if int(file[5:7]) == 58:\n",
    "                    print(file)\n",
    "                    input_video = os.path.join(root, file)\n",
    "                    face_extractor.run(input_video,output_folder_path)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)Transform 7 frames to a tensor, and save as a sampleLib list. \n",
    "\n",
    "* input_folder_path = '/.../oulu_face_extracted'\n",
    "* output_path = '/.../oulu_trans_video_list.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_tensor(input_video):\n",
    "    transform=transforms.Compose([  transforms.ToPILImage(),\n",
    "    transforms.Resize(256),  \n",
    "                                transforms.CenterCrop(224),  # crop to 224\n",
    "                                transforms.ToTensor()])\n",
    "    tensor_list = []\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    for i in range(int(cap.get(7))):\n",
    "        ret, frame = cap.read()\n",
    "        frame_trans = transform(frame)\n",
    "        frame_trans = torch.unsqueeze(frame_trans, 0)\n",
    "        tensor_list.append(frame_trans)\n",
    "    video_tensor = torch.cat(tensor_list,dim=0) \n",
    "    return video_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    input_folder_path = '/.../oulu_face_extracted'\n",
    "    label_dict = {'A':0,'D':3,'F':4,'H':5,'1':1,'2':2}\n",
    "\n",
    "    video_list = []\n",
    "    \n",
    "    for root,dirs,files in os.walk(input_folder_path):\n",
    "        for file in files:\n",
    "            input_video = os.path.join(root, file)\n",
    "            tensor_list  = video_tensor(input_video)\n",
    "            label_class = input_video[-5]\n",
    "            label = label_dict[label_class]\n",
    "            if int(tensor_list.shape[0]) !=7:\n",
    "                print(file)\n",
    "            video_list.append([tensor_list,label])\n",
    "    torch.save(video_list, '/.../oulu_trans_video_tensor_list.pkl')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}